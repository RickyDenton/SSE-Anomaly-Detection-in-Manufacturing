"""Module containing the CandidateModelEvaluation class
implementing the model anomalydetectionandperformance and selection phase of the pipeline."""
import logging
import os
import pickle
import sys

from json import JSONDecodeError
from json import dumps
import coloredlogs
import numpy
import pandas
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report
from sklearn.model_selection import PredefinedSplit
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn_evaluation import plot
from pkg_resources import resource_filename
from jsonschema import ValidationError, SchemaError
from configuration import Configuration


class CandidateModelEvaluation:
    """Class representing the data preparation step of the pipeline.

        Functions:
            inizialize_logger(): Creates the logger used by the class for print output
            _retrieve_configuration(): Imports the training_configuration
            _retrieve_data_sets(): Imports and builds the data sets
            _create_and_save_model(): Trains and selects the best model to predict responses
            _create_and_save_diagrams(): ....
            _create_and_save_reports(): ....
            start(): performs the candidate selection phase
        Attributes:
            _logger (Logger): Class' logger to produce formatted output
            _configuration_path (str): Path from which the configuration will be loaded
            _training_configuration (Configuration): The configuration of the service.
            _data_sets (dict): A dictionary containing all the data sets
            _candidate_model (MLPClassifier): The final model generated by the GridSearchCV
            _y_prediction(-----): .....
        """

    def initialize_logger(self):
        """Initialize the logger."""
        self._logger = logging.getLogger(__name__)

        handler = logging.StreamHandler(sys.stdout)
        formatter = coloredlogs.ColoredFormatter("%(asctime)s %(name)s"
                                                 " %(levelname)s %(message)s",
                                                 "%Y-%m-%d %H:%M:%S")

        handler.setFormatter(formatter)

        self._logger.addHandler(handler)
        self._logger.setLevel(logging.INFO)

    def _retrieve_configuration(self) -> bool:
        """Import the training configuration from a fixed path and perform its validation

        Returns:
            True: the configuration is correctly extracted and validated
            False: an error has occurred during the training importation
        """
        self._logger.info("Retrieving the training configuration file..")
        try:
            self.training_configuration = Configuration(self._configuration_path['conf_path'],
                                                        self._configuration_path['schema_path'])

            param_hls = self.training_configuration.field['parameters']["hidden_layer_sizes"].copy()
            hls = [tuple(h) for h in param_hls]

            self.training_configuration.field['parameters']["hidden_layer_sizes"] = hls
            self._logger.info("Training configuration correctly loaded")
            return True

        except OSError:
            self._logger.error("Error during the training_configuration loading."
                               " Configuration not present")
        except JSONDecodeError:
            self._logger.error("Error during the training_configuration "
                               "loading. Invalid format")
        except ValidationError:
            self._logger.error("Error during "
                               "the training_configuration loading. Invalid configuration")
        except SchemaError:
            self._logger.error("Error during the json schema. Invalid format")

        return False

    def _retrieve_data_sets(self) -> bool:
        """Import the training, testing and validation sets from a fixed path taken from
           the training configuration and perform their preparation.

        Returns:
            True: data sets are correctly imported and ready to be used
            False: an error has occurred during the data sets importation
        """

        if self._data_sets is not None:
            return True

        self._logger.info("Retrieving of the data "
                          "sets for the machine learning development..")
        training_path = resource_filename(
            self.training_configuration.field['general_information']['source_pool_location'],
            self.training_configuration.field['general_information']['source_train_path'])
        testing_path = resource_filename(
            self.training_configuration.field['general_information']['source_pool_location'],
            self.training_configuration.field['general_information']['source_test_path'])
        validation_path = resource_filename(
            self.training_configuration.field['general_information']['source_pool_location'],
            self.training_configuration.field['general_information']['source_valid_path'])

        if not os.path.exists(training_path) or \
                not os.path.exists(testing_path) or \
                not os.path.exists(validation_path):
            self._logger.error("Data sets not found")
            return False

        try:
            self._logger.info("Data sets correctly loaded, starting rebuilding of data..")
            training_set = pandas.read_csv(training_path)
            testing_set = pandas.read_csv(testing_path)
            validation_set = pandas.read_csv(validation_path)

        except ValueError:
            self._logger.error("Error during data sets importation")
            return False

        self._logger.info("Data sets rebuilt")
        self._data_sets = {
            "training_set": {
                "X": training_set.loc[:, training_set.columns != 'ANOMALOUS'],
                "Y": training_set['ANOMALOUS']
            },
            "validation_set": {
                "X": validation_set.loc[:, validation_set.columns != 'ANOMALOUS'],
                "Y": validation_set['ANOMALOUS']
            },
            "testing_set": {
                "X": testing_set.loc[:, testing_set.columns != 'ANOMALOUS'],
                "Y": testing_set['ANOMALOUS']
            }
        }

        self._logger.info("Data sets correctly imported")
        return True

    def _create_and_save_model(self) -> bool:
        """Generate an optimum MLPClassifier by a GridSearchCV selection and save it into
           the file system

        Returns:
            True: the model is correctly generated and exported as .sav file
            False: an error has occurred during the training of the model
        """
        self._logger.info("Starting candidate selection..\n\n")

        # creation of the training/validation data sets
        training_validation_x = numpy.concatenate(
            (self._data_sets['training_set']['X'], self._data_sets['validation_set']['X']))
        training_validation_y = numpy.concatenate(
            (self._data_sets['training_set']['Y'], self._data_sets['validation_set']['Y'])).ravel()

        # creation of the train/validation sets split, -1 means training_set 0 validation_set
        test_fold = [-1 * (i < self._data_sets['training_set']['X'].shape[0]) for i in
                     range(self._data_sets['training_set']['X'].shape[0] +
                           self._data_sets['validation_set']['X'].shape[0])]
        # creation of the machine learning model for anomalies classification
        val = MLPClassifier(
            max_iter=self.training_configuration.field['training_conf']['training_epochs'],
            tol=self.training_configuration.field['training_conf']['tol'],
            max_fun=self.training_configuration.field['training_conf']['max_fun'])
        # performing of the model training/candidate selection. GridSearch is configured
        # to not use k-fold analysis and instead use a training/validation set
        self.candidate_model = GridSearchCV(
            estimator=val,
            verbose=1,
            param_grid=self.training_configuration.field['parameters'],
            n_jobs=self.training_configuration.field['training_conf']['n_jobs'],
            cv=PredefinedSplit(test_fold))
        try:
            self.candidate_model.fit(training_validation_x, training_validation_y)
            print('\n', flush=True)
        except ValueError:
            self._logger.error("Error during the training, invalid configuration")
            return False

        self._logger.info("Candidate model generation completed.")
        return True

    def _create_and_save_diagrams(self) -> bool:
        """Generate the hystogram with the mean score of every models 
           analyzed by means of the grid search
        Returns:
            True: every diagram drawn correctly
            False: candidate model not found
        """
        out_path = resource_filename(
            __name__,
            self.training_configuration.field['general_information']['dest_pool_location']
        )

        if not self.candidate_model:
            module_path = out_path + "candidate_model.sav"
            if not os.path.exists(module_path):
                return False
            self.candidate_model = pickle.load(open(module_path, "rb"))

        grid_search_result = plot.grid_search(
            self.candidate_model.cv_results_, change='hidden_layer_sizes', kind='bar'
        )

        grid_search_result.get_legend().set_bbox_to_anchor(bbox=(1.05, 1))
        grid_search_result.get_legend().loc = 'upper right'
        grid_search_result \
            .get_figure(). \
            savefig(out_path + 'grid_search_models_results.png', bbox_inches="tight")
        plt.clf()

        test_y = self._data_sets["testing_set"]["Y"]
        target_names = ['correct process', 'suspected malfunction']
        self.y_prediction = self.candidate_model.predict(self._data_sets["testing_set"]["X"])
        plot.confusion_matrix(
            test_y,
            self.y_prediction,
            target_names
        ).get_figure().savefig(out_path + 'Conf_matrix.png', bbox_inches="tight")
        plt.clf()
        return True

    def _create_and_save_reports(self) -> bool:
        """produces two files containing respectively best model hyperparameters and best model evaluation metrics

         Returns:
            True: every file created correctly
            False: candidate model not found
        """
        out_path = resource_filename(
            __name__,
            self.training_configuration.field['general_information']['dest_pool_location']
        )

        if not self.candidate_model:
            module_path = out_path + "candidate_model.sav"
            if not os.path.exists(module_path):
                return False
            self.candidate_model = pickle.load(open(module_path, "rb"))

        if self.y_prediction.any():
            self.y_prediction = self.candidate_model.predict(self._data_sets["testing_set"]["X"])

        # y_score = self.candidate_model.predict_proba(self._data_sets["testing_set"]["X"])

        target_names = ['correct process', 'suspected malfunction']

        test_y = self._data_sets["testing_set"]["Y"]
        y_pred = self.y_prediction

        model_report = classification_report(
            y_true=test_y,
            y_pred=y_pred,
            target_names=target_names,
            zero_division=0
        )
        with open(out_path + 'best_model.json', 'w') as file:
            file.write(dumps(self.candidate_model.best_params_))
        with open(out_path + 'report_metrics.txt', 'w') as file:
            file.write(model_report)
        return True

    def __init__(self):
        # logger configuration
        self._logger = None
        self.initialize_logger()
        # creation of configuration paths for the training_configuration import
        self._configuration_path = {
            "conf_path": resource_filename(__name__, "configuration/configuration.json"),
            "schema_path": resource_filename(__name__, "configuration/configuration.schema.json")
        }
        # definition of class variables
        self.training_configuration = None
        self._data_sets = None

        self.candidate_model = None
        self.y_prediction = None

    def start(self) -> bool:
        """Function to start the module functionality. It performs sequentially all the phases
            of the Model Evaluation and Candidate Selection pipeline stage

        Returns:
            True: if the application has correctly performed all its phases
            False: if an error has occurred during the execution
        """
        # we leave to the user the duty to identify if the model is sufficiently valid
        # or a new selection has to be performed by looking to the reports data
        # generated into the output folder
        while True:
            if self._retrieve_configuration() and \
                    self._retrieve_data_sets() and \
                    self._create_and_save_model() and \
                    self._create_and_save_diagrams() and \
                    self._create_and_save_reports():
                answer = input("CANDIDATE MODEL EVALUATION: Do you want to reload the "
                               "training configuration and repeat the CANDIDATE GENERATION? [y/n] ")
                if answer.lower() == 'n' or answer.lower() == "no":
                    model_path = resource_filename(
                        __name__,
                        self.training_configuration.field['general_information']
                        ['dest_pool_location']
                    )
                    pickle.dump(
                        self.candidate_model,
                        open(model_path + "candidate_model.sav", "wb"))
                    self._logger.info("Candidate model exported as "
                                      "candidate_model.sav into the output folder")
                    return True
                continue
            return False
